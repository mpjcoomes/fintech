Credit Scoring
=

Lending between individuals and organisations allows mutual benefit for those with and without capital. Whenever an organisation lends to a credit applicant the price is determined by many factors (e.g. transaction costs, the business environment, central bank interest rates, time period). One of the most important is the risk that the applicant, once accepted, will default on the credit, resulting in an organisational loss of the principal.

Credit risk assessment is the field of technical expertise used by lenders to achieve 'fair discrimination' by producing a full and unbiased default risk estimate (Anderson, 2007, p.16). Accurate assessment is essential if lenders are to treat credit applicants in accord with previous choices and behaviours. It permits lenders to appropriately price credit for low-risk applicants (e.g. stable employment), reject high-risk applicants (e.g. repeated prior defaults), and build new financial products suited for specific markets (e.g. short-term bridging loans).

Credit scoring data sets used by lenders include applicant personal details, variables predictive of default risk, and application status (i.e. accepted or rejected). Accepted applicants have 'real outcomes' or future performance information (i.e. default or not default). Rejected applicants, being rejected and thus lacking the opportunity to default, have no real outcomes.

Lenders develop credit scoring models using data from past applicants. A response variable, 'real outcome' in this case, is required for model training. The training data represents a censored sample of the target population because performance information is only available for accepted applicants. Using exclusively accepted applicants means that rejected applicants, and any misclassified (i.e. falsely rejected) low-default subgroups therein, are underrepresented in future scorecards.

If records were 'missing at random' (Rubin, 1976) there would be no statistical or econometric difference between the sample and the 'Through-The-Door' (TTD) population. Accepted applicants are different from rejected applicants because default risk, lending policies, and government regulations determine selection. It is systematic, not random. This causes sample selection bias, and consequently, biased estimates for the models predicting a TTD population that were parameterised on an accepted applicant population (Banasik, Crook, & Thomas, 2003; Finlay, 2012, p.234-239). As Makuch (2001) summarises, due to "the guidelines used in originating the population, the resulting data are censored and, if only theoretically, represent those applicants believed to be good credit risks" (p.134).

Models trained on censored data may inadvertently reject low-risk applicants and accept high-risk applicants. Experienced loan officers can accept applicants who, despite having parameters indicating high default risk, do not default due to their unique characteristics. A model trained on these data will have a distorted sample on which to compute the default risk associated with applicant characteristics. Parameters evidencing high-risk in reality indicate low-risk in the model, causing systematic misclassification of TTD populations (Maldonado & Paredes, 2010, p.561). Accordingly, rejected applicants represent non-ignorable missing data values (Qin, Leung, & Shao, 2002), and the composition of training data is determined by the previous decision-making infrastructure.

## Reject Inference

Lenders attempt to correct this bias with Reject Inference Methods (RIMs). These are processes by which data sets are 'treated' to compensate for the unavailability of real outcome performance data for rejected applicants.

Of note is that RIMs have the potential to reduce sampling bias in any application with censored or biased data (Verstraeten & Van den Poel, 2005). It yields competitive advantage, given than the pre-existing depth of model sophistication remains unchanged while sampling bias is corrected. Comparing RIMs under experimental conditions (i.e. ceteris paribus) requires a standardised classification model that can be used interchangeably with each method. The performance of the RIM used, and the initial extent of the bias, determines the potential for model improvement (Banasik & Crook, 2010).

## Application Scoring

Within the credit scoring field there are 'application' and 'behavioural' scoring models.

Application scoring is for credit applicants. It uses information submitted to lenders by applicants themselves, and information from debt bureaus, to produce a one-off, point of application estimate of default likelihood over a given time period. Behavioural scoring is for customers. It uses the account information generated by ongoing customer behaviour to forecast future behaviour. Credit has already been granted, the lender is committed for the duration, so behavioural scoring is used to forecast default risk and thereby manage the exposure (Bijak & Thomas, 2012; Kennedy et al., 2013). Only application scoring is relevant for this research. The experimental data consists exclusively of data available at the point of application and predictions consist of one-off estimates of default risk.

The appropriate statistical framework is 'classification'. This is an old modelling paradigm with a mature research literature. Approaches to binary classification (e.g. 1 vs. 0, default vs. not default) are legion within the credit scoring discipline. Literature reviews (Crook, Edelman, & Thomas, 2007; Crook, Thomas, & Edelman, 2012) show that potential application scoring options include Neural Networks, *k*-Nearest Neighbour clustering, Genetic Algorithms, Support Vector Machines (SVM), and ensemble methods such as Random Forests (RF) and Gradient Boosting Machines (GBM). This range can be expanded with bespoke combinations of the above into hybrid classification methods that aggregate predictions from different types of models (Reddy & Ravi, 2013). All algorithms have fine tuning parameters that alter performance.

These advanced classifiers usually outperform traditional approaches in a laboratory environment. However, it does not follow that they are therefore part of the branch-level industry toolkit. It is challenging for in-house scorecard developers to replicate performance established in a laboratory on propriety market-sourced data sets. SVM, for example, has been long noted as a suitable contender for business application. However, it has high data-quality requirements that rarely present in research, and which can prevent it from making meaningful improvements to predictive accuracy when applied to real-world data (Crook et al., 2007).

## Regression

Recent research does not support the promise that SVM will even produce superior models. When data sets are large and balanced, meaning they have sufficient defaulting records, traditional Logistic Regression (LOG) models remain competitive (Musa, 2013). Brown and Mues (2012) show that SVM performs poorly against LOG and another traditional classifier, Linear Discriminant Analysis (DA), with both LOG and DA being competitive against all modern alternatives save recent ensemble algorithms.

These benchmarking results do not provide a compelling rationale for lenders to dismantle existing institutionalised modelling systems and adopt modern alternatives. The transition would be costly and deliver limited performance gains, especially for lenders possessing large data sets that can employ traditional classifiers (e.g. LOG) competitively.

For RIM comparisons to be generalised to branch practice the classification model must resemble what is presently in use. Of the above classifiers, several textbook references assert that LOG is the backbone of industry application scoring (Anderson, 2007, p.58; Siddiqi, 2006, p.89-90; Thomas, Edelman, & Crook, 2002, p.41). After decades of industry acceptance, "logistic regression and linear programming" (i.e. mathematical optimisation) are the "two main stalwarts of today's card builders" (Thomas, 2000, p.152). LOG is almost universally included in benchmarking research of modern classifiers as the industry standard against which alternatives are judged.

LOG can model relationships between a dichotomous response variable, typically coded in binary form (e.g. 1 vs. 0, default vs. not default), and one or more regressors. A reason for the popularity of LOG in classification problems is the logistic function, ƒ(z) = 1 / (1+ℯ<sup>-z</sup>) where -∞ ≤ z ≤ ∞ and 0 ≤ ƒ(z) ≤ 1.

This alters parameter estimation as compared to Linear Regression (LR) and allows LOG to output likelihood predictions as a probability of the modelled event occurring (Kleinbaum & Klein, 2010, p.5).

LR estimates parameters arithmetically by minimising the sum-of-squared deviations between predicted and actual values (i.e. least squares). In contrast, LOG transforms regressors into a logistic function and iteratively solves for the best parameters using Maximum Likelihood Estimation (MLE). Parameters are 'guessed' (i.e. starting values) then iteratively adjusted to maximise log likelihood until the method converges on an optimal solution. As Hox (2010) summarises, the estimates produced by MLE "are those parameter estimates that maximize the probability of finding the sample data that we have actually found" (p.16).

There are three main reasons LOG overtook the LR alternative and came to dominate industry scorecard development. The first is the noted facility to output robust and accurate probabilities for a dichotomous response variable (Thomas, Edelman, & Crook, 2002, p.65). The second is that LOG has less restrictive assumptions. Because the predicted values are binary, 'linear probability modelling' (i.e. using LR for classification) violates the assumption of normally distributed and homoscedastic residuals. It is also prone to outputting impossible probability values (e.g. negatives). In contrast, LOG has no assumptions about regressor distributions and cannot output probabilities beyond the 0 to 1 range (Tabachnick & Fidell, 2013, p.439). LOG only requires that relationships be linear within the logistic function, that residuals be independent, and the absence of multicollinearity (i.e. where regressors are related by a linear function; Everitt & Skrondal, 2010, p.287). In practice, even these LOG assumptions are violated, yet LOG remains a suitable option, or realistically a 'less unsuitable' one.

The third reason LOG usage grew was advances in computer processing power (Crook, Thomas, & Edelman, 2012). The MLE used to estimate LOG parameters avoids some assumptions, restrictions and biases, at the cost of greater computing time. According to Anderson (2007), this was "infeasible at a time when computers were big and slow", but today differences in computational time are "hardly noticeable", and consequently LOG "is used by between 80 and 90 per cent of scorecard developers" (p.165). LOG is a relatively fast model to train on current hardware, especially given that leading alternatives are computationally intensive machine-learners. Eventually those alternatives may reduce the need for reject inference altogether (e.g. 'Transductive' SVM; Maldonado & Paredes, 2010). The steep trajectory of computing power is informative as to how the branch-level toolkit may evolve. Whatever future industry preferences may be, LOG is at present the industry standard, and was consequently used in application scoring models to compare RIMs.

## Parcelling

Only the Anderson (2007, p.407-408) presentation of RIMs starts with an explanation of 'parcelling'. Understanding this element of reject inference clarifies much of the confusion and overlap between methods. Most RIMs, and classification algorithms in general, do not output 'crisp' categorisations for predictions. Rather, models classifying rejected applicants as default or not default output the 'fuzzy' probability of class membership. Parcelling refers to the technique, which can typically be separated from the RIM entirely, that a modeller uses to exploit these predicted probabilities for optimal classification.

PRP parcelling reclassifies rejects based on the predicted probability for each record being either above or below a set cut-off value. If default is being predicted then rejects above the cut-off are classified as defaulting and remaining rejects as not. Lenders train the final LOG model on an amalgamated data set of classified rejects and accepted applicants. The cut-off value is set by fixed rules (e.g. 50% records), default rate adjusted by future lending strategy (e.g. conservative or liberal), or optimised by iteratively maximising an accuracy measure (i.e. method of conjugate gradients). All methods introduce extraneous elements into any RIM comparison. The most transparent and 'least damaging' approach is to fix the cut-off at the proportion of defaults in rejected applicants. In practice, lenders would need to estimate this information from debt bureau reports on current default rates or 'buy' the true rate by accepting a random sample of rejected applicants.

FUZ is often used as the parcelling technique for EXT. By duplicating and classifying all rejects, then weighting them by predicted probability (i.e. 'probability of true'), all rejects can be included in training data without having to set a hard probability cut-off or estimate the default rate of rejected applicants.

NID replicates how branch-modellers may discard 'indeterminate' records from the training data (Siddiqi, 2006, p.43-45; Anderson, 2007, p.409). Predicted probabilities can cluster at the extremes of the predicted range. The model is outputting a high probability of class membership for records estimated to lie within these risk regions. Likewise, the model is outputting a lower probability of class membership for records scattered between extremes, the indeterminate records. NID replicates the branch-modelling approach of discarding indeterminates. A replicable alternative is to sort applicants then classify those in the top 20% and bottom 20% of the predicted probability distribution.

|Method|Summary|
|:---|:---|
|PRP: Proportional Polarised parcelling<sup>(1,2,3)</sup>|Predict the default probability of rejects using some model of accepts (i.e. AccApps, Super Models) or of applicants (i.e. Memory Based Reasoning). Sort rejects by predicted probabilities then classify them in proportion to the known default rate. Train a final LOG on the combined data set.|
|FUZ: Fuzzy parcelling<sup>(2,3)</sup>|Add two copies of rejects to the training data. Classify one copy as defaults and the other as not defaults. Weight accepts as 1 and classified rejects as the probability of default or not default.|
|NID: No Indeterminates parcelling|Classify and combine rejects with extreme predicted probabilities. Discard 'indeterminate' rejects.|

*Reference Key. Thomas, 2009<sup>(1)</sup>; Anderson, 2007<sup>(2)</sup>; Siddiqi, 2006<sup>(3)</sup>.*


## Classification Models

The guiding principle for all models constructed should be that they are "explicit and plausible" (Crook & Banasik, 2004, p.873). This approach facilitates objective comparison of RIMs on a data set incorporating multiple sources and scoring systems.

There are four common classification models. One is a basic LOG of a priori and BVs trained on accepted applicants only. Another two are 'Super Models', so named "because of their seemingly extremely high predictive power" (Anderson, p.414). They are designed for inferring rejected applicants only and consequently are freed of the restrictions associated with a branch-lending scorecard (e.g. number of variables). The most predictive BVs, as identified by a [variable importance GBM](https://github.com/mpjcoomes/machine-learning/blob/d4297176ee7f742d9a5b181d7247d8b279802bd7/MLVariableSelection.md), can be modelled using a 'Super Logistic Regression' (SLOG) and a 'Super Gradient-Booting Machine' (SGBM).

Memory Based Reasoning (MBR) is the fourth classification model. MBR uses a nonparametric machine-learning algorithm called *k*-Nearest Neighbours. It classifies records in a specified data set using the distance of each from the *k* closest other records (i.e. 'nearest neighbours') in a training data set (Hastie, Tibshirani, & Friedman, 2009, p.14). It is optimised by manipulating classification method (e.g. majority vote, weighted), *k* value, and distance measure. Early approaches used majority voting that reduced neighbour information to crisp classifications (Siddiqi, 2006, p.111-112). Recent approaches (Finlay, 2012, p.249) use the default rate among the accepted applicant neighbours as a predicted probability of default. This latter form allows MBR to be tested with PRP, FUZ and NID parcelling. Some of the remaining RIMs below incorporate parcelling, only not in an isolated manner as per models above.

## Reject Inference Methods

The purpose of an application scoring LOG is to predict default risk of future 'Through-The-Door' (TTD) credit applicants. This requires training data of TTD applicants that includes real outcome information (i.e. default or not default) as the focus of modelling. Accepted applicants were granted credit and possess real outcome information. Rejected applicants, who had no opportunity to default, lack real outcomes and cannot be used for training unaltered. Any resulting application scoring LOG would be trained on a portion of TTD applicants that was systematically selected by the previous decision-making infrastructure. This introduces sampling bias, because the population to be predicted includes all TTD credit applicants (i.e. accepted and rejected), while the training data includes only accepted applicants. A statistical model is unreliable when predicting a population alien to that for which it was trained.

Lenders utilise 'Reject Inference Methods' (RIMs) to salvage rejected applicants and thereby mitigate bias (Verstraeten & Van den Poel, 2005). Reject inference is "the process of inferring the status of applicants who have been rejected" (Hand & Henley, 1993). The goal of any given RIM is to address sampling bias by generating 'inferred performance' for rejected applicant records as if they had been accepted (Anderson, 2007). The transformation results in training data that more closely resembles a TTD applicant profile.

Reject inference was presented in econometric research over two decades ago (Boyes, Hoffman, & Low, 1989). Common RIMs are included in leading credit scoring textbooks as a necessity of application scoring (Thomas, Edelman, & Crook, 2002; Siddiqi, 2006; Anderson, 2007; Thomas, 2009; Finlay, 2012).

Terminology and application procedures conflict between authors, so RIMs are summarised below for consistency within this analysis. To provide context, the summary shows the three 'non-RIM' LOGs first. Because all resources refer to elements of each RIM only explicit mentions warranted referencing. Further details on individual RIMs are provided afterwards in order of presentation.

|Method|Summary|
|:---|:---|
|AllApps: All Applicants|LOG model trained on uncensored TTD applicants (i.e. hypothetically perfect RIM, least bias).|
|AccApps: Accepted Applicants<sup>(4)</sup>|Only accepts without rejects or any RIM (i.e. no reject inference, greatest bias).|
|SampAllApps: Sampled All Applicants|AllApps with equal training data size to AccApps via a random sample of TTD applicants.|
|ARD: All Rejects Default<sup>(4,5)</sup>|Classify all rejects as defaults and train the final LOG on the combined data set.|
|AUG: Augmentation<sup>(1,2,3,4,5)</sup>|AccApps, with records weighted by the probability of rejection (e.g. Reject or Accept LOG).|
|BCR: Boundary Cluster Reclassification|Cluster applicants. Identify cluster default risk using AccApps predicted probabilities. Classify and combine rejects from low-risk and high-risk clusters. Discard rejects from 'indeterminate' clusters.|
|EXT: Extrapolation<sup>(1,2,3,5)</sup>|Use default risk trends in accepts to infer, and then adjust for (e.g. 'Bivariate Inference', 'Uniform Shift'), the distortion in predicted probabilities of rejects. Combine with accepts using FUZ.|
|ITR: Iterative Reclassification<sup>(1,3,4)</sup>|Classify rejects using AccApps and parcelling. Train a LOG on the combined data set. Repeatedly reclassify and parcel rejects using each reparameterised LOG until one solution converges.|
|Random Supplementation<sup>(1,2,3,4)</sup>|Accept 2,000 applicants that would otherwise have been rejected, absorbing upfront costs of default to buy future reductions in sampling bias. Either 'True' (TRS), where 'rejects' are sampled completely at random, or 'Marginal' (MRS), where default risk is capped by a set threshold.|
|TGA: Three-Group Approach<sup>(5)</sup>|Multinomial prediction of applicants into rejects, defaults or not defaults. Combined using FUZ.|

*Reference Key. Finlay, 2012<sup>(1)</sup>; Thomas, 2009<sup>(2)</sup>; Anderson, 2007<sup>(3)</sup>; Siddiqi, 2006<sup>(4)</sup>; Thomas, Edelman, & Crook, 2002<sup>(5)</sup>.*

The first three non-RIM LOGs apply no reject inference. Each contributes to the RIM comparison by using a different training data set. 'All Applicants' (AllApps) is trained on an uncensored TTD applicant data set that would be unavailable in a branch-lending environment. 'Accepted Applicants' (AccApps) is trained exclusively on accepted applicants. This implies, when predicting a TTD applicant population, the lowest sampling bias for AllApps and the highest for AccApps. The assessment standard for each RIM is the extent it improves model performance from AccApps to AllApps (i.e. from baseline to optimal).

AllApps includes rejected applicant records and consequently has a larger training data set than AccApps. Observed differences between models could be an artefact of the total number of records available for training rather than training data composition. The 'Sampled All Applicants' (SampAllApps) non-RIM LOG was included to explore this possibility. SampAllApps uses random sampling without replacement to train a LOG model equivalent to AllApps in data set composition and to AccApps in data set size.

ARD is the 'crudest' RIM. The modeller summarily classifies all rejects as defaults "on the grounds that there must have been bad information about them for them to have been rejected previously" (Thomas, Edelman, & Crook, 2002, p.141-142). This RIM is suitable for high-risk lending where the high default rate, and low number of rejected applicants, mitigates the misclassification implicit in such an arbitrary approach (Siddiqi, 2006, p.102).

AUG is an AccApps LOG weighted by rejection likelihood. AUG assumes that "marginal accepts" are similar to "marginal rejects". Both marginally accepted applicants and rejected applicants have a high probability of rejection (Ash & Meester, 2002). Weighting records by probability of rejection up-weights those that were 'almost rejected'. This specialises the model for new applicants straddling the default risk cut-off. The resulting LOG has less sampling bias in the marginal risk regions that influence lending decisions (Finlay, 2012, p.243). All applicants are used to produce a predicted probability of rejection for each accepted applicant. This can be done by directly modelling rejection as a response variable using LOG (p.243). Another option is to cluster all applicants and set rejection rate within each cluster as the probability of rejection (p.249).

BCR uses unsupervised clustering to reclassify rejected applicants in low and high risk clusters. It is present in journal literature rather than textbooks. Chen, Xiang, Liu and Wang (2012) use *k*-Means to identify and reclassify cases based on default risk. Hsieh (2005) use Self-Organising Maps (SOM) to identify and remove "unrepresentative" samples that cause sampling bias in training data, although the author did not cite reject inference. The Hsieh approach is lengthy and requires a more directed research focus than possible in this project. Chen et al. is replicable, although authors still had to repeatedly apply the *k*-Means algorithm, assess clusters, and discard those that did not "consistently" model default. To automate this process and make it suitable for this project, a resampling approach that repeatedly applies the similar yet more robust *k*-Mediods algorithm was used. Clustering is already applied for AUG weighting and in MBR. BCR adds a baseline for a straightforward application of clustering that is underrepresented in research.

EXT is defined differently across references. This confusion may stem from whether predicted probabilities are adjusted to compensate for sampling bias. Finlay (2012, p.244-247) describes EXT as AccApps FUZ parcelling with adjusted probabilities. Anderson (2007, p.412) describes it as any parcelling technique, and discusses adjustment separately as 'Bivariate Inference' (p.415-416). An early definition of EXT is "extrapolating a model built on the accepted applicants into the reject region" (Hand & Henley, 1993, p.45). This is clarifying, because an unadjusted accepted applicant model will underestimate the predicted probability of default in high-rejection regions. To continue accepted applicant trends into this region, the probability of default for rejects must be adjusted to offset sampling bias.

Several methods are in use to adjust probabilities. Some modellers use line fitting methods to model the reject region, which Finlay (2012) describes as "a mixture of a science and an art" (p.247). Two replicable alternatives tested in this project are 'Uniform Shift' and 'Bivariate Inference'. Uniform Shift adjusts all probabilities by a fixed amount. Finlay recommends adding .1 to the predicted probability of default of all rejects. Bivariate Inference involves multiplying the estimated probability of not default by the estimated probability of acceptance, as determined by an AccApps LOG and a rejection LOG respectively. This inflates the probability of default proportionally to the probability of rejection, meaning to the extent of the sampling bias.

The origin of 'Bivariate Inference' must be briefly addressed. 'Tobit' regression models deal with statistical problems involving unobservable variables (Tobin, 1958). Early reject inference research focussed on joint distribution models, including 'bivariate probit with sample selection' (Meng & Schmidt, 1985), 'bivariate censored probit' (Boyes, Hoffman, & Low, 1989), and Heckman's Two-Step Method (Heckman, 1976; 1979). Relative to AccApps type LOGs, bivariate probit models exhibit minor improvements in accuracy (Banasik, Crook, & Thomas, 2003; Kim & Sohn, 2007) or no change (Banasik & Crook, 2007). Wu and Hand (2007) conclude that even under optimal assumptions a Heckman Two-Step model does not result in meaningful improvement.

Cited textbooks no longer advocate separate bivariate probit models, focussing instead on direct adjustment of probabilities. Whenever the probability of rejection is used, the adjustment is typically called 'Bivariate Inference' or Heckman's Bias Correction (Ash & Meester, 2002). In summary, EXT should refer to AccApps under FUZ parcelling, where predicted probabilities for default are adjusted either with a 'Uniform Shift' or with 'Bivariate Inference'.

ITR, originally from Joanes (1993), applies "fractional allocation of the rejects" (p.35) using an iteratively applied application scoring LOG. It involves using the final prediction LOG produced from a parcelled AccApps model to reclassify previously classified rejects. The LOG is then retrained on the updated combined data set. ITR repeats this process (i.e. iterates) until the LOG parameters converge on one solution, which should occur within ten iterations (Finlay, p.248).

Random supplementation methods are equivalent to 'buying' data and the main drawback is cost. Finlay (2012, p.240) demonstrates with a cost estimate scenario. If 500 of 2,000 randomly accepted applicants default, with a $2000 average loss for defaulting applicants and $200 average profit otherwise, projected losses are $700,000 (1,500\*$200 – 500\*$2,000). These hypothetical valuations permit similar calculations in the Results, such that observed bias reductions can be placed in the context of upfront costs. Authors called this RIM 'random', yet in practice, only 'accepted rejects' below an upper limit of default risk are sampled. Finlay specifies that "accepted rejects only need to be taken from the region where the good:bad odds are greater than one quarter of the cut-off odds" (p.240). The cited hypothetical limit is a good:bad odds cut-off of 8:1 to 2:1. True randomness would include the whole population of rejected applicants. Therefore, two RIMs to consider are Marginal Random Supplementation (MRS), where default risk is capped, and True Random Supplementation (TRS).

TGA requires the response variable to be restructured so that all applicants are divided "into three groups: goods, bads, and rejects" (Thomas, Edelman, & Crook, 2002, p.143). It uses a multinomial logistic model from Reichert, Cho and Wagner (1983). Unfortunately, there is limited information on how TGA ought to be applied. Thomas et al. (2002) present TGA as a branch alternative to LOG. They express concern as to how lenders deal with having scorecards that predict both default and rejection. Nevertheless, TGA has unique advantages for reject inference. By using multinomial modelling, the predicted probability of applicant rejection is already provided, thus avoiding the need for a separate LOG to predict rejection status. This allows creation of two TGA-based RIMs. First, the unaltered probabilities for default and not default can be used directly for FUZ parcelling. This approach is unbalanced because probability of reject status is discarded. Therefore, as an alternative, the predicted probability for reject status can be used to adjust default probabilities, as per 'Bivariate Inference' in EXT.

'Data Surrogacy' (Finlay, 2012, p.240-243, aka. 'In-House' or 'Bureau Data Based Method', Siddiqi, 2006, p.104-105; 'Cohort Performance', Anderson, 2007, p.413-414) should be avoided for test data because it requires additional applicant information only available to lenders. Data Surrogacy assumes customers defaulting in other credit products (e.g. loan repayments) will default on the inferred product (e.g. credit cards). Rejected applicant performance in other accounts can 'surrogate' for real outcomes.

## RIM Performance

The research literature provides some indication of expected RIM performance. However, it focuses on AUG, EXT, ITR and 'Bivariate Inference'. This may reflect the extent of use in industry. Anderson (2007) asserts that other "techniques have been suggested, but it is impossible to say how extensively they are being used in practice" (p.410). Some authors cite AUG and ITR as the most prominent RIMs (Verstraeten & Van den Poel, 2005; Finlay, 2012, p.251). This focus is reflected in the paucity of research on other methods.

Despite wide adoption, both AUG and EXT exhibit poor performance in the literature. Crook and Banasik (2004) compare these RIMs using propriety data. They conclude that EXT is "both useless and harmless". Similarly, AUG provides no advantages over an unweighted model and can impair a model where rejection rates are high. This is supported in general by other papers (Banasik & Crook, 2005; 2007; 2010), although the performance of AUG may be higher when the reject model predicts high probabilities of rejection and the records with the highest probabilities have the poorest credit performance. Chen and Astebro (2006) conclude that the "traditional augmentation model is... inferior to ignoring missing data" (p.18).

'Bivariate Inference' has mixed results. Banasik, Crook and Thomas (2003) are positive, showing that a small accuracy improvement is possible when loan officers have overruled the scoring rule (i.e. high reject probabilities). Chen and Astebro (2006) are negative, concluding that 'bivariate adjustment' applied with PRP "does not work well for reject inference" (p.18) and repeat that ignoring missing data is superior. Similarly, research on separate bivariate probit models with sample selection slightly improved AUC (Kim & Sohn, 2007), or were neutral or counterproductive (Banasik & Crook, 2007).

An early review of RIMs by Hand and Henley (1993) is prescient. They concluded that all RIMs have burdensome statistical assumptions and variability in how they are applied. The assumption requirements undermine the theoretical justification that the methods can improve predictions and suggest they could make them worse. That is, there is no sound statistical principle to support RIMs improving predictive performance. While this does not prevent RIMs from improving performance, it does reframe them as novel data treatment methods, with potential to both improve and hinder the accuracy of decisioning. This emphasises the importance of assessing all models with an isolated TTD test set to expose the practical consequences of poor RIMs.

The concern over assumptions in early research emphasises the more recent nonparametric RIMs. In particular, the clustering approaches have potential for wide application, especially in non-normal data sets where traditional RIMs are counterproductive. Both *k*-Means clustering (Chen, Xiang, Liu & Wang, 2012) and SOM (Hsieh, 2005) have, at least, proof of concept for potential effectiveness. There is support for MBR as a competitive RIM (Anderson, Haller, Siddiqi, Cox, & Duling, 2010). However, this evidence is restricted to the SAS implementation of the *k*-Nearest Neighbour algorithm within a 'SAS Enterprise Miner' (SAS Institute Inc., 2012) process flow and custom macro. Corroboration of this performance under an open-source implementation would be informative.

There is limited information on random supplementation methods. However, out of all the methods, this has the strongest theoretical foundation. According to Thomas (2009), the best remedy to the sampling bias, "in statistical if not in economic terms", is to "accept a random sample of all applicants just to acquire information on their subsequent status" (p.65). Finlay (2012) concludes, putting aside all other RIMs, that the "best way to address sample bias is to accept some applications that should be rejected" (p.238). Nevertheless, as stated above, the high costs of this method may neutralise any performance gains.

## Conclusion

An important point from the above review is that the sampling bias will be minor, at least to the extent it can be detected within an experimental paradigm. The presence of some sampling bias is strongly supported, yet as Berlin and Mester (2004) write, summarising Crook and Banasik (2004), the "actual effect of selection bias is relatively small", and "the potential gains in accuracy from using the full sample instead of the censored sample are relatively small". When comparing RIMs, this necessitates both precision and a variety of assessments to capture the effects of each in terms that are meaningful on the branch level.

## References

Anderson, B., Haller, S., Siddiqi, N., Cox, J., Duling, D. (2010). Improving Credit Risk Scorecards with Memory-Based Reasoning to Reject Inference with SAS® Enterprise Miner™. SAS Global Forum: SAS Institute Inc.

Anderson, R. (2007). The Credit Scoring Toolkit: Theory and Practice for Retail Credit Risk Management and Decision Automation. New York, USA: Oxford University Press.

Ash, D., Meester, S. (2002). Best Practices in Reject Inferencing. Paper presented at Conference on Credit Risk Modeling and Decisioning: Wharton FIC, University of Pennsylvania.

Banasik, J., Crook, J. (2005). Credit scoring, augmentation and lean models. Journal of the Operational Research Society, 56, 1072-1081.

Banasik, J., Crook, J. (2007). Reject inference, augmentation, and sample selection. European Journal of Operational Research, 183, 1582-1594.

Banasik, J., Crook, J. (2010). Reject inference in survival analysis by augmentation. Journal of the Operational Research Society, 61, 473-485.

Banasik, J., Crook, J., Thomas, L. (2003). Sample selection bias in credit scoring models. Journal of the Operational Research Society, 54, 822-832.

Berlin, M., Mester, L.J. (2004). Retail credit risk management and measurement: An introduction to the special issue. Journal of Banking & Finance, 28, 721-725.

Bijak, K., Thomas, L.C. (2012). Does segmentation always improve model performance in credit scoring? Expert Systems with Applications, 39, 2433-2442.

Boyes, W., Hoffman, D., Low, S. (1989). An econometric analysis of the bank credit scoring problem. Journal of Econometrics, 40, 3-14.

Brown, I., Mues, C. (2012). An experimental comparison of classification algorithms for imbalanced credit scoring data sets. Expert Systems with Applications, 39, 3446-3453.

Chen, G., Astebro, T.B. (2006). A Maximum Likelihood Approach for Reject Inference in Credit Scoring. Rotman School of Management Working Paper, 07-05. Retrieved March, 2021, http://papers.ssrn.com/sol3/papers.cfm?abstract_id=872541

Chen, W., Xiang, G., Liu, Y., Wang, K. (2012). Credit risk Evaluation by hybrid data mining technique. Systems Engineering Procedia, 3, 194-200.

Crook, J., Banasik, J. (2004). Does reject inference really improve the performance of application scoring models? Journal of Banking & Finance, 28, 857-874.

Crook, J., Edelman, D.B., Thomas, L.C. (2007). Recent developments in consumer credit risk assessment. European Journal of Operational Research, 183, 1447-1465.

Crook, J., Thomas, L., Edelman, D. (2012). Editorial. International Journal of Forecasting, 28, 128-132.

Everitt, B.S., Skrondal, A.S. (2010). The Cambridge Dictionary of Statistics (4th ed.). Cambridge, UK: Cambridge University Press.

Finlay, S. (2012). Sample Bias and Reject Inference. In Credit Scoring, Response Modeling, and Insurance Rating: A Practical Guide to Forecasting Consumer Behavior (2nd ed.). UK: Palgrave Macmillan.

Hand, D.J., Henley, W.E. (1993). Can reject inference ever work? IMA Journal of Mathematics Applied in Business & Industry, 5, 45-55.

Hastie, T., Tibshirani, R., Friedman, J.H. (2009). The Elements of Statistical Learning: Data Mining, Inference, and Prediction (2nd ed.). New York: Springer.

Heckman, J.T. (1976). The Common Structure of Statistical Models of Truncation, Sample Selection and Limited Dependent Variables and a Simple Estimator for Such Models. Annals of Economic and Social Measurement, 5, 475-492.

Heckman, J.T. (1979). Sample Selection Bias as a Specification Error. Econometrica, 47, 153-161.

Hox, J. (2010). Multilevel Analysis: Techniques and Applications (2nd ed.). Great Britain, UK: Routledge.

Hsieh, N.C. (2005). Hybrid mining approach in the design of credit scoring models. Expert Systems with Applications, 28, 655-665.

Joanes, D.N. (1993). Reject inference applied to logistic regression for credit scoring. IMA Journal of Management Math, 5, 35-43.

Kennedy, K., Mac Namee, B., Delany, S.J., O’Sullivan, M., Watson, N. (2013). A window of opportunity: Assessing behavioural scoring. Expert Systems with Applications, 40, 1372-1380.

Kim, Y., Sohn, S.Y. (2007). Technology scoring model considering rejected applicants and effect of reject inference. Journal of the Operational Research Society, 58, 1341-1347.

Kleinbaum, D.G, Klein, M. (2010). Logistic Regression: A Self‐Learning Text (3rd ed.). New York, USA: Springer.

Makuch, W.M. (2001). Chapter 7 The Basics of Better Application Score. In E. Mays (Ed.), Handbook of Credit Scoring, 134-135. Chicago, Illinois, USA: Glenlake.

Maldonado, S., Paredes, G. (2010). A Semi-supervised Approach for Reject Inference in Credit Scoring Using SVMs Advances in Data Mining. Applications and Theoretical Aspects Lecture Notes in Computer Science Volume, 6171, 558-571.

Meng, C.L., Schmidt, P. (1985). On the cost of partial observability in the bivariate probit model. International Economic Review, 26, 71-85.

Musa, A.B. (2013). Comparative study on classification performance between support vector machine and logistic regression. International Journal of Machine Learning and Cybernetics, 4, 13-24.

Qin, J., Leung, D., Shao, J. (2002). Estimation with Survey Data under Nonignorable Nonresponse or Informative Sampling. Journal of the American Statistical Association, 97, 193-200.

Reddy, K.N., Ravi, V. (2013). Differential evolution trained kernel principal component WNN and kernel binary quantile regression: Application to banking. Knowledge-Based Systems, 39, 45-56.

Reichert, A.K., Cho, C.C., Wagner, G.M. (1983). An examination of the conceptual issues involved in developing credit scoring models. Journal of Business and Economic Statistics, 1, 101-114.

Rubin, D. (1976). Inference and Missing Data. Biometrika, 63, 581-592.

Siddiqi, N. (2006). Credit Risk Scorecards: Developing and Implementing Intelligent Credit Scoring. New Jersey, USA: John Wiley & Sons, Inc.

Statistical Analysis System (SAS) Institute Inc. (2012). SAS Enterprise Miner. SAS Institute Inc., Cary, NC, USA.

Tabachnick, B.G., Fidell, L.S. (2013). Using Multivariate Statistics (6th ed.). USA: Pearson Education, Inc.

Thomas, L.C. (2000). A survey of credit and behavioural scoring: forecasting financial risk of lending to consumers. International Journal of Forecasting, 16, 149-172.

Thomas, L.C. (2009). Consumer Credit Models: Pricing, Profit, and Portfolios. New York, USA: Oxford University Press.

Thomas, L.C., Edelman, D.B., Crook, J.N. (2002). Credit Scoring and its Applications. Philadelphia, USA: SIAM.

Tobin, J. (1958). Estimation of Relationships for Limited Dependent Variables. Econometrica, 26, 24-36.

Verstraeten, G., Van den Poel, D. (2005). The Impact of Sample Bias on Consumer Credit Scoring Performance and Profitability. Journal of the Operational Research Society, 56, 981-992.

Wu, I.D., Hand, D.J. (2007). Handling selection bias when choosing actions in retail credit applications. European Journal of Operational Research, 183, 1560-1568.

